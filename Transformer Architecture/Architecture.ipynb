{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-26T15:02:46.310203Z",
     "start_time": "2025-12-26T15:02:46.298793Z"
    }
   },
   "source": "#npip install accelerate",
   "outputs": [],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:02:47.167326Z",
     "start_time": "2025-12-26T15:02:47.154980Z"
    }
   },
   "cell_type": "code",
   "source": "# pip install transformers",
   "id": "32f2e37ab3aad519",
   "outputs": [],
   "execution_count": 178
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:02:48.038493Z",
     "start_time": "2025-12-26T15:02:48.027228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "6c716641d53dbefc",
   "outputs": [],
   "execution_count": 179
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading the LLM",
   "id": "1556287a1909d9b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### first load the model and its tokenizer.\n",
    "\n",
    "For that we will first import the classes:\n",
    "\n",
    "\n",
    "AutoModelForCausalLM and AutoTokenizer. When we want to process a\n",
    "sentence, we can apply the tokenizer first and then the model in two\n",
    "separate steps. Or we can create a pipeline object that wraps the two\n",
    "steps and then apply the pipeline to the sentence.\n",
    "\n",
    "This is why you'll also import the pipeline class.\n",
    "\n"
   ],
   "id": "1f8bfce23eaae5c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The transformers library has two types of model classes:\n",
    "AutoModelForCausalLM and AutoModelForMaskedLM.\n",
    "\n",
    "Causal language models represent the decoder-only models that are used\n",
    "for text generation.\n",
    "\n",
    "\n",
    "They are described as causal, because to predict the next token, the\n",
    "model can only attend to the preceding left tokens.\n",
    "\n",
    "\n",
    "Masked language models represent the encoder-only models that are used\n",
    "for rich text representation.\n",
    "\n",
    "\n",
    "They are described as masked, because they are trained to predict a\n",
    "masked or hidden token in a sequence.\n"
   ],
   "id": "c69ea09400491dbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:02:51.909814Z",
     "start_time": "2025-12-26T15:02:51.898781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import the required classes\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ],
   "id": "33e9194663663284",
   "outputs": [],
   "execution_count": 180
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:02:52.807742Z",
     "start_time": "2025-12-26T15:02:52.798998Z"
    }
   },
   "cell_type": "code",
   "source": "import torch",
   "id": "8ad26de9157832f9",
   "outputs": [],
   "execution_count": 181
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T10:26:16.240055Z",
     "start_time": "2025-12-26T10:26:16.230080Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer, AutoModelForCausalLM",
   "id": "3d904b0ec2f07197",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:02:54.680213Z",
     "start_time": "2025-12-26T15:02:54.562582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/microsoft/Phi-3-mini-4k-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"models/microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ],
   "id": "10d2316b44ed9aeb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 108.67it/s]\n"
     ]
    }
   ],
   "execution_count": 182
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:03:21.147137Z",
     "start_time": "2025-12-26T15:03:21.112555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    # False means to not include the prompt text in the returned text\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False,\n",
    "    # no randomness in the generated text\n",
    ")"
   ],
   "id": "af7dd3ca27eb3d5d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "execution_count": 184
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:03:50.191474Z",
     "start_time": "2025-12-26T15:03:22.563060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"models/microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "out = generator(\n",
    "    \"Write an email apologizing to Sarah for the tragic gardening mishap.\",\n",
    "    max_new_tokens=60,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "print(out[0][\"generated_text\"])"
   ],
   "id": "c851648cc9dbb95b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 222.22it/s]\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write an email apologizing to Sarah for the tragic gardening mishap.\n",
      "\n",
      "\n",
      "Subject: My Deepest Apologies for the Gardening Mishap\n",
      "\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "\n",
      "I hope this message finds you in good spirits despite the unfortunate events that have unfolded in our shared garden. I am writing to express my sincerest\n"
     ]
    }
   ],
   "execution_count": 185
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Model Architecture**",
   "id": "f73d0b22156e9371"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:03:59.593367Z",
     "start_time": "2025-12-26T15:03:59.563242Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "77f1a047337941c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 186
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:04:13.129318Z",
     "start_time": "2025-12-26T15:04:01.971242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get model output - use the full model, not just model.model\n",
    "# This handles the cache properly\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    outputs = model(input_ids, use_cache=False)\n",
    "\n",
    "# Get logits (output before softmax)\n",
    "logits = outputs.logits\n",
    "print(f\"\\nLogits shape: {logits.shape}\")"
   ],
   "id": "f654bdd40d50274b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logits shape: torch.Size([1, 5, 32064])\n"
     ]
    }
   ],
   "execution_count": 187
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:04:13.157368Z",
     "start_time": "2025-12-26T15:04:13.131253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the last token's logits (predictions for next token)\n",
    "last_token_logits = logits[0, -1, :]\n",
    "print(f\"Last token logits shape: {last_token_logits.shape}\")"
   ],
   "id": "9ca02a3fd1c58d04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last token logits shape: torch.Size([32064])\n"
     ]
    }
   ],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:04:13.189890Z",
     "start_time": "2025-12-26T15:04:13.168021Z"
    }
   },
   "cell_type": "code",
   "source": "model.model.embed_tokens",
   "id": "e044a8ff4008410c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32064, 3072, padding_idx=32000)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:04:13.214296Z",
     "start_time": "2025-12-26T15:04:13.191961Z"
    }
   },
   "cell_type": "code",
   "source": "model.model",
   "id": "2fa1f90ae17826b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3Model(\n",
       "  (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "  (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x Phi3DecoderLayer(\n",
       "      (self_attn): Phi3Attention(\n",
       "        (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "        (rotary_emb): Phi3RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Phi3MLP(\n",
       "        (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        (activation_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): Phi3RMSNorm()\n",
       "      (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (post_attention_layernorm): Phi3RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): Phi3RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 190
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:04:13.238968Z",
     "start_time": "2025-12-26T15:04:13.218112Z"
    }
   },
   "cell_type": "code",
   "source": "model.model.layers[0]",
   "id": "f7c14b62f05e912a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3DecoderLayer(\n",
       "  (self_attn): Phi3Attention(\n",
       "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (mlp): Phi3MLP(\n",
       "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "    (activation_fn): SiLUActivation()\n",
       "  )\n",
       "  (input_layernorm): Phi3RMSNorm()\n",
       "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_attention_layernorm): Phi3RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 191
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T14:20:29.802032Z",
     "start_time": "2025-12-26T14:20:29.771855Z"
    }
   },
   "cell_type": "code",
   "source": "model.model.embed_tokens",
   "id": "76925d94031736df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32064, 3072, padding_idx=32000)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 164
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generating a Single Token to a Prompt\n",
   "id": "4d45b2de6637f6e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "earlier used the Pipeline object to generate a text response to a prompt. The pipeline provides an abstraction to the underlying process of text generation. Each token in the text is actually generated one by one.\n",
    "\n",
    "Let's now give the model a prompt and check the first token it will generate.\n"
   ],
   "id": "70aceb3c5a24f635"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:05:16.959211Z",
     "start_time": "2025-12-26T15:05:16.948364Z"
    }
   },
   "cell_type": "code",
   "source": "prompt = \"The capital of France is\"",
   "id": "4f46072c86ada775",
   "outputs": [],
   "execution_count": 203
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:04:13.309874Z",
     "start_time": "2025-12-26T15:04:13.285512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "print(f\"Input IDs: {input_ids}\")\n",
    "print(f\"Input shape: {input_ids.shape}\")"
   ],
   "id": "2f3b8dfded687245",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 450, 7483,  310, 3444,  338]])\n",
      "Input shape: torch.Size([1, 5])\n"
     ]
    }
   ],
   "execution_count": 193
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:04:25.450397Z",
     "start_time": "2025-12-26T15:04:15.186745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get model output - use the full model, not just model.model\n",
    "# This handles the cache properly\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    outputs = model(input_ids, use_cache=False)"
   ],
   "id": "59ec0ad56ce0f6ae",
   "outputs": [],
   "execution_count": 194
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:04:25.461050Z",
     "start_time": "2025-12-26T15:04:25.452176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the last token's logits (predictions for next token)\n",
    "last_token_logits = logits[0, -1, :]\n",
    "print(f\"Last token logits shape: {last_token_logits.shape}\")"
   ],
   "id": "db0c17d15215f6a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last token logits shape: torch.Size([32064])\n"
     ]
    }
   ],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:04:25.477094Z",
     "start_time": "2025-12-26T15:04:25.470397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the token ID with highest probability\n",
    "next_token_id = last_token_logits.argmax(-1)\n",
    "print(f\"\\nNext token ID: {next_token_id}\")\n",
    "\n",
    "# Decode the token\n",
    "next_token = tokenizer.decode(next_token_id)\n",
    "print(f\"Next token: '{next_token}'\")\n",
    "\n",
    "# Show the complete sequence\n",
    "print(f\"\\nComplete: {prompt}{next_token}\")"
   ],
   "id": "f0309258e06eb6de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next token ID: 3681\n",
      "Next token: 'Paris'\n",
      "\n",
      "Complete: The capital of France isParis\n"
     ]
    }
   ],
   "execution_count": 196
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:04:35.996895Z",
     "start_time": "2025-12-26T15:04:25.486810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Access model layers manually\n",
    "print(\"\\n--- Accessing Model Layers Manually ---\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get hidden states from the base model\n",
    "    model_output = model.model(\n",
    "        input_ids,\n",
    "        use_cache=False,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "# Last hidden state before lm_head\n",
    "hidden_states = model_output.last_hidden_state\n",
    "print(f\"Hidden states shape: {hidden_states.shape}\")\n",
    "\n",
    "# Apply lm_head manually\n",
    "lm_head_output = model.lm_head(hidden_states)\n",
    "print(f\"LM head output shape: {lm_head_output.shape}\")\n",
    "\n",
    "# Get next token (should match previous result)\n",
    "token_id = lm_head_output[0, -1].argmax(-1)\n",
    "print(f\"Next token (manual): '{tokenizer.decode(token_id)}'\")"
   ],
   "id": "42350188528d2a9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Accessing Model Layers Manually ---\n",
      "Hidden states shape: torch.Size([1, 5, 3072])\n",
      "LM head output shape: torch.Size([1, 5, 32064])\n",
      "Next token (manual): 'Paris'\n"
     ]
    }
   ],
   "execution_count": 197
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:04:36.023788Z",
     "start_time": "2025-12-26T15:04:36.014447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the shape the output the model before the lm_head\n",
    "model_output[0].shape"
   ],
   "id": "736c695008d172df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3072])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 198
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The first number represents the batch size, which is 1 in this case since we have one prompt. The second number 5 represents the number of tokens. And finally 3072 represents the embedding size (the size of the vector that corresponds to each token).\n",
    "\n",
    "Let's now get the output of the LM head."
   ],
   "id": "1e52ab688e5dab19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:04:36.043082Z",
     "start_time": "2025-12-26T15:04:36.031978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the output of the lm_head\n",
    "lm_head_output = model.lm_head(model_output[0])"
   ],
   "id": "e42ce7c4402b097f",
   "outputs": [],
   "execution_count": 199
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:04:36.057161Z",
     "start_time": "2025-12-26T15:04:36.043837Z"
    }
   },
   "cell_type": "code",
   "source": "lm_head_output.shape",
   "id": "f66cb28b4dc4b943",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32064])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 200
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The LM head outputs for each token in the input prompt, a vector of size 32064 (vocabulary size).\n",
    "\n",
    "So there are 5 vectors, each of size 32064. Each vector can be mapped to a probability distribution,\n",
    "that shows the probability for each token in the vocabulary to come after the given token in the input prompt.\n",
    "\n",
    "Since we're interested in generating the output token that comes after the last token in the input prompt (\"is\"), we'll focus on the last vector.\n",
    "\n",
    "So in the next cell, lm_head_output[0,-1] is a vector of size 32064 from which you can generate the token that come\n",
    "after (\"is\").\n",
    "\n",
    "we can do that by finding the id of the token that corresponds to the highest value in the vector\n",
    "lm_head_output[0,-1] (using argmax(-1), -1 means across the last axis here).\n"
   ],
   "id": "c79b83ed2191dde2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:04:36.077604Z",
     "start_time": "2025-12-26T15:04:36.058956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_id = lm_head_output[0,-1].argmax(-1)\n",
    "token_id"
   ],
   "id": "c881f06225b24e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3681)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 201
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:05:27.245552Z",
     "start_time": "2025-12-26T15:05:27.217299Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(token_id)",
   "id": "f94354c7c636620b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 204
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
