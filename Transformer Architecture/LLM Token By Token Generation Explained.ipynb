{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T18:06:11.758127Z",
     "start_time": "2025-12-26T17:36:53.911482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def generate_text(prompt, model, tokenizer, max_new_tokens=100, show_progress=True):\n",
    "    \"\"\"\n",
    "    Generate text using manual token-by-token generation.\n",
    "    Avoids cache compatibility issues.\n",
    "\n",
    "    Args:\n",
    "        prompt: Input text prompt\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        max_new_tokens: Maximum number of tokens to generate\n",
    "        show_progress: Whether to print generation progress\n",
    "\n",
    "    Returns:\n",
    "        Generated text (string)\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    generated_ids = input_ids.clone()\n",
    "\n",
    "    if show_progress:\n",
    "        print(f\"Generating up to {max_new_tokens} tokens...\")\n",
    "\n",
    "    generated_text = \"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_new_tokens):\n",
    "            # Forward pass\n",
    "            outputs = model(generated_ids, use_cache=False)\n",
    "\n",
    "            # Get next token\n",
    "            next_token_id = outputs.logits[0, -1, :].argmax(-1)\n",
    "\n",
    "            # Decode\n",
    "            token_text = tokenizer.decode(next_token_id)\n",
    "            generated_text += token_text\n",
    "\n",
    "            # Progress\n",
    "            if show_progress and (i + 1) % 20 == 0:\n",
    "                print(f\"  [{i + 1} tokens]\")\n",
    "\n",
    "            # Update sequence\n",
    "            generated_ids = torch.cat([\n",
    "                generated_ids,\n",
    "                next_token_id.unsqueeze(0).unsqueeze(0)\n",
    "            ], dim=1)\n",
    "\n",
    "            # Check for end\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                if show_progress:\n",
    "                    print(f\"  [Stopped at token {i + 1}]\")\n",
    "                break\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# USAGE EXAMPLES\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE 1: Simple generation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "prompt1 = \"Write a short info about the LLM\"\n",
    "output1 = generate_text(prompt1, model, tokenizer, max_new_tokens=100)\n",
    "\n",
    "print(f\"\\nPrompt: {prompt1}\")\n",
    "print(f\"\\nGenerated:\\n{output1}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 2: Different prompt\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "prompt2 = \"The capital of France is\"\n",
    "output2 = generate_text(prompt2, model, tokenizer, max_new_tokens=50)\n",
    "\n",
    "print(f\"\\nPrompt: {prompt2}\")\n",
    "print(f\"\\nGenerated:\\n{output2}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 3: Code generation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "prompt3 = \"Write a Python function to calculate factorial:\"\n",
    "output3 = generate_text(prompt3, model, tokenizer, max_new_tokens=150)\n",
    "\n",
    "print(f\"\\nPrompt: {prompt3}\")\n",
    "print(f\"\\nGenerated:\\n{output3}\")\n",
    "\n",
    "\n",
    "# ADVANCED: With temperature sampling\n",
    "def generate_text_sampling(prompt, model, tokenizer, max_new_tokens=100,\n",
    "                          temperature=1.0, top_k=50, show_progress=True):\n",
    "    \"\"\"\n",
    "    Generate text with sampling for more creative outputs.\n",
    "\n",
    "    Args:\n",
    "        prompt: Input text prompt\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        max_new_tokens: Maximum number of tokens to generate\n",
    "        temperature: Controls randomness (higher = more random)\n",
    "        top_k: Only sample from top k most likely tokens\n",
    "        show_progress: Whether to print progress\n",
    "\n",
    "    Returns:\n",
    "        Generated text (string)\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    generated_ids = input_ids.clone()\n",
    "\n",
    "    if show_progress:\n",
    "        print(f\"Generating (temp={temperature}, top_k={top_k})...\")\n",
    "\n",
    "    generated_text = \"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_new_tokens):\n",
    "            outputs = model(generated_ids, use_cache=False)\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "\n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply top-k filtering\n",
    "            if top_k > 0:\n",
    "                top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
    "                logits_filtered = torch.full_like(logits, float('-inf'))\n",
    "                logits_filtered[top_k_indices] = top_k_logits\n",
    "            else:\n",
    "                logits_filtered = logits\n",
    "\n",
    "            # Sample from distribution\n",
    "            probs = torch.softmax(logits_filtered, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Decode\n",
    "            token_text = tokenizer.decode(next_token_id)\n",
    "            generated_text += token_text\n",
    "\n",
    "            # Update\n",
    "            generated_ids = torch.cat([\n",
    "                generated_ids,\n",
    "                next_token_id.unsqueeze(0)\n",
    "            ], dim=1)\n",
    "\n",
    "            # Check for end\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 4: Creative sampling (temperature=0.8)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "prompt4 = \"Once upon a time\"\n",
    "output4 = generate_text_sampling(\n",
    "    prompt4, model, tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.8,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(f\"\\nPrompt: {prompt4}\")\n",
    "print(f\"\\nGenerated:\\n{output4}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n"
   ],
   "id": "6b736e60f943cd1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMPLE 1: Simple generation\n",
      "======================================================================\n",
      "Generating up to 100 tokens...\n",
      "  [20 tokens]\n",
      "  [40 tokens]\n",
      "  [60 tokens]\n",
      "  [80 tokens]\n",
      "  [100 tokens]\n",
      "\n",
      "Prompt: Write a short info about the LLM\n",
      "\n",
      "Generated:\n",
      "modelknownasGPT-3.GPT-3,whichstandsforGenerativePre-trainedTransformer3,isanautoregressivelanguagemodelthatusesdeeplearningtoproducehuman-liketext.DevelopedbyMicrosoft,itisthethirditerationoftheGPT-nseriesandoneofthemostadvancedlanguagemodelsavailable.GPT-3has175billionparameters,whicharethepartsofthemodelthatarelearnedfromthetrainingdata.\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: Different prompt\n",
      "======================================================================\n",
      "Generating up to 50 tokens...\n",
      "  [Stopped at token 17]\n",
      "\n",
      "Prompt: The capital of France is\n",
      "\n",
      "Generated:\n",
      "Paris.\n",
      "\n",
      "\n",
      "###Response:ThecapitalofFranceisParis.<|endoftext|>\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 3: Code generation\n",
      "======================================================================\n",
      "Generating up to 150 tokens...\n",
      "  [20 tokens]\n",
      "  [40 tokens]\n",
      "  [60 tokens]\n",
      "  [80 tokens]\n",
      "  [100 tokens]\n",
      "  [120 tokens]\n",
      "  [140 tokens]\n",
      "\n",
      "Prompt: Write a Python function to calculate factorial:\n",
      "\n",
      "Generated:\n",
      "\n",
      "\n",
      "Input:\n",
      "factorial(n)\n",
      "\n",
      "Output:\n",
      "Thefactorialofn(n!)\n",
      "\n",
      "Input:\n",
      "factorial(5)\n",
      "\n",
      "Output:\n",
      "120\n",
      "\n",
      "Input:\n",
      "factorial(0)\n",
      "\n",
      "Output:\n",
      "1\n",
      "\n",
      "Input:\n",
      "factorial(-1)\n",
      "\n",
      "Output:\n",
      "Error:Factorialisnotdefinedfornegativenumbers.\n",
      "\n",
      "Input:\n",
      "factorial(3.5)\n",
      "\n",
      "Output:\n",
      "Error:Factorialisnotdefinedfornon-integervalues.\n",
      "\n",
      "Input:\n",
      "factorial(10)\n",
      "\n",
      "Output:\n",
      "3628800\n",
      "\n",
      "Input:\n",
      "factorial(100)\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 4: Creative sampling (temperature=0.8)\n",
      "======================================================================\n",
      "Generating (temp=0.8, top_k=50)...\n",
      "\n",
      "Prompt: Once upon a time\n",
      "\n",
      "Generated:\n",
      "inabusycitylivedasmall,vibrantflowershopownedbyayoungwomannamedLily.Shehadalwaysdreamedofrunningasuccessfulbusinessbutoftenfeltoverwhelmedbythechallengesofmanagingeverythingonherown.\n",
      "\n",
      "Oneday,shemetawiseoldmentornamedRose.Rosehadflourishedintheflowerindustryfordecadesandhadseenitall.LilydecidedtoaskRoseforadviceandguidanceonhowtomakeher\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T16:45:41.879722Z",
     "start_time": "2025-12-26T16:37:30.287708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer (if not already loaded)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"models/microsoft/Phi-3-mini-4k-instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"models/microsoft/Phi-3-mini-4k-instruct\",\n",
    "#     device_map=\"cpu\",\n",
    "#     torch_dtype=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "\n",
    "# WORKING METHOD: Manual Token-by-Token\n",
    "prompt2 = \"Write a short info about the LLM\"\n",
    "max_new_tokens = 100\n",
    "\n",
    "print(f\"Prompt: {prompt2}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt2, return_tensors=\"pt\").input_ids\n",
    "generated_ids = input_ids.clone()\n",
    "\n",
    "print(f\"\\nGenerating up to {max_new_tokens} tokens...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "generated_text = \"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(max_new_tokens):\n",
    "        # Forward pass (no cache)\n",
    "        outputs = model(generated_ids, use_cache=False)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Get the next token ID\n",
    "        next_token_id = logits[0, -1, :].argmax(-1)\n",
    "\n",
    "        # Decode and accumulate\n",
    "        token_text = tokenizer.decode(next_token_id)\n",
    "        generated_text += token_text\n",
    "\n",
    "        # Show progress\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"[{i + 1}/{max_new_tokens} tokens generated]\")\n",
    "\n",
    "        # Add to sequence\n",
    "        generated_ids = torch.cat([\n",
    "            generated_ids,\n",
    "            next_token_id.unsqueeze(0).unsqueeze(0)\n",
    "        ], dim=1)\n",
    "\n",
    "        # Stop if EOS token is generated\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            print(f\"\\n[Stopped early at token {i + 1} - End of sequence]\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GENERATED TEXT:\")\n",
    "print(\"=\" * 70)\n",
    "print(generated_text)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Total tokens generated: {len(tokenizer(generated_text).input_ids)}\")\n",
    "print(\"=\" * 70)"
   ],
   "id": "d38c0f89ff375a7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a short info about the LLM\n",
      "======================================================================\n",
      "\n",
      "Generating up to 100 tokens...\n",
      "----------------------------------------------------------------------\n",
      "[20/100 tokens generated]\n",
      "[40/100 tokens generated]\n",
      "[60/100 tokens generated]\n",
      "[80/100 tokens generated]\n",
      "[100/100 tokens generated]\n",
      "\n",
      "======================================================================\n",
      "GENERATED TEXT:\n",
      "======================================================================\n",
      "modelknownasGPT-3.GPT-3,whichstandsforGenerativePre-trainedTransformer3,isanautoregressivelanguagemodelthatusesdeeplearningtoproducehuman-liketext.DevelopedbyMicrosoft,itisthethirditerationoftheGPT-nseriesandoneofthemostadvancedlanguagemodelsavailable.GPT-3has175billionparameters,whicharethepartsofthemodelthatarelearnedfromthetrainingdata.\n",
      "\n",
      "======================================================================\n",
      "Total tokens generated: 118\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T16:35:54.907694Z",
     "start_time": "2025-12-26T16:27:16.642474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = generate_text(\n",
    "    \"Write a short info about the LLM\",\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_new_tokens=100\n",
    ")\n",
    "print(output)"
   ],
   "id": "f2b335108b2371ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating up to 100 tokens...\n",
      "  [20 tokens]\n",
      "  [40 tokens]\n",
      "  [60 tokens]\n",
      "  [80 tokens]\n",
      "  [100 tokens]\n",
      "modelknownasGPT-3.GPT-3,whichstandsforGenerativePre-trainedTransformer3,isanautoregressivelanguagemodelthatusesdeeplearningtoproducehuman-liketext.DevelopedbyMicrosoft,itisthethirditerationoftheGPT-nseriesandoneofthemostadvancedlanguagemodelsavailable.GPT-3has175billionparameters,whicharethepartsofthemodelthatarelearnedfromthetrainingdata.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T16:27:16.620914Z",
     "start_time": "2025-12-26T16:18:58.005748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = generate_text_sampling(\n",
    "    \"Once upon a time\",\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.8\n",
    "    # Higher = more creative\n",
    ")\n",
    "print(output)"
   ],
   "id": "488448edace859a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating (temp=0.8, top_k=50)...\n",
      ",inasmallvillagenestledatthefootofamajesticmountain,therelivedayounggirlnamedMaya.Mayawasknownthroughoutthevillageforheradventurousspiritandherinsatiablecuriosity.Shehadaparticularinterestintheworldofelectronics,fascinatedbytheirabilitytoconnectpeopleandbringtheworldclosertogether.\n",
      "\n",
      "Oneday,asMayawasexploringthebustlingmarketsquare,shestumbleduponapeculiarsight.A\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
