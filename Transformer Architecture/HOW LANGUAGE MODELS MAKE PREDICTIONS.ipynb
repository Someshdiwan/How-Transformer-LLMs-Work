{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-26T15:15:06.599405Z",
     "start_time": "2025-12-26T15:13:55.466562Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "üéØ HOW LANGUAGE MODELS MAKE PREDICTIONS\n",
    "A visual, step-by-step explanation with real numbers\n",
    "\n",
    "the complete journey from input text to final prediction\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# SETUP\n",
    "print(\"\\n\" + \"üß†\" * 35)\n",
    "print(\"   HOW DOES THE AI BRAIN MAKE PREDICTIONS?\")\n",
    "print(\"üß†\" * 35 + \"\\n\")\n",
    "\n",
    "MODEL_PATH = \"/Users/somesh/How Transformer LLMs Work/Transformer Architecture/models/microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"‚úÖ Model loaded!\\n\")\n",
    "\n",
    "# STEP-BY-STEP PREDICTION PROCESS\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìù INPUT PROMPT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(\"\\nLet's see how the model predicts the next word...\\n\")\n",
    "\n",
    "input(\"\\n‚è∏Ô∏è  Press ENTER to see Step 1: Tokenization...\")\n",
    "\n",
    "\n",
    "# STEP 1: TOKENIZATION\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üî§ STEP 1: TOKENIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "tokens = [tokenizer.decode([id]) for id in input_ids[0]]\n",
    "\n",
    "print(\"\\nüìã The model breaks text into tokens:\")\n",
    "print(\"-\"*80)\n",
    "for i, (token, token_id) in enumerate(zip(tokens, input_ids[0])):\n",
    "    print(f\"  Position {i}: '{token}' ‚Üí Token ID: {token_id.item()}\")\n",
    "\n",
    "print(f\"\\nüí° Total input tokens: {len(tokens)}\")\n",
    "print(\"   Each token gets a unique ID number that the model understands\")\n",
    "\n",
    "input(\"\\n‚è∏Ô∏è  Press ENTER to see Step 2: Forward Pass...\")\n",
    "\n",
    "\n",
    "# STEP 2: FORWARD PASS THROUGH MODEL\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß† STEP 2: PROCESSING THROUGH THE MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nThe model is now thinking...\")\n",
    "print(\"  ‚Ä¢ Converting tokens to embeddings (3,072 dimensions)\")\n",
    "print(\"  ‚Ä¢ Processing through 32 transformer layers\")\n",
    "print(\"  ‚Ä¢ Each layer refines the understanding\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, use_cache=False)\n",
    "    logits = outputs.logits  # Raw predictions\n",
    "\n",
    "print(\"\\n‚úÖ Processing complete!\")\n",
    "print(f\"   Output shape: {logits.shape}\")\n",
    "print(f\"   Meaning: {logits.shape[0]} batch √ó {logits.shape[1]} tokens √ó {logits.shape[2]} vocab\")\n",
    "\n",
    "input(\"\\n‚è∏Ô∏è  Press ENTER to see Step 3: Understanding Logits...\")\n",
    "\n",
    "# STEP 3: UNDERSTANDING LOGITS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä STEP 3: UNDERSTANDING LOGITS (Raw Scores)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get logits for the last token (the one we want to predict after)\n",
    "last_token_logits = logits[0, -1, :]  # Shape: [32064]\n",
    "\n",
    "print(f\"\\nüéØ For the last token ('{tokens[-1]}'), the model outputs:\")\n",
    "print(f\"   A vector of {len(last_token_logits):,} numbers (one for each possible word)\")\n",
    "print(\"\\nüìà What are logits?\")\n",
    "print(\"   Raw, unnormalized scores indicating how 'good' each word would be next\")\n",
    "print(\"   Higher logit = model thinks this word is more likely\")\n",
    "\n",
    "print(\"\\nüîç Let's look at some example logits:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Show a few random logits\n",
    "sample_indices = [0, 100, 1000, 10000, 20000, 31000]\n",
    "for idx in sample_indices:\n",
    "    sample_word = tokenizer.decode([idx])\n",
    "    sample_logit = last_token_logits[idx].item()\n",
    "    print(f\"   Word: '{sample_word}' (ID: {idx:5d}) ‚Üí Logit: {sample_logit:8.3f}\")\n",
    "\n",
    "print(\"\\nüí° Notice: Logits can be negative or positive, any size!\")\n",
    "print(\"   We need to convert these to probabilities...\")\n",
    "\n",
    "input(\"\\n‚è∏Ô∏è  Press ENTER to see Step 4: Converting to Probabilities...\")\n",
    "\n",
    "\n",
    "# STEP 4: SOFTMAX (LOGITS ‚Üí PROBABILITIES)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé≤ STEP 4: CONVERTING LOGITS TO PROBABILITIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüî¨ SOFTMAX FUNCTION:\")\n",
    "print(\"   Converts any set of numbers into probabilities (0 to 1, sum to 1)\")\n",
    "print(\"\\n   Formula: probability(word_i) = e^(logit_i) / Œ£(e^(logit_j) for all j)\")\n",
    "\n",
    "# Apply softmax\n",
    "probabilities = torch.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "print(\"\\n   Before Softmax (Logits):\")\n",
    "print(\"   [-5.2, 8.7, -1.3, 0.5, 3.2, ...]\")\n",
    "print(\"          ‚Üì  SOFTMAX  ‚Üì\")\n",
    "print(\"   After Softmax (Probabilities):\")\n",
    "print(\"   [0.0001, 0.8730, 0.0034, 0.0205, 0.0318, ...]\")\n",
    "\n",
    "print(f\"\\n‚úÖ Softmax applied!\")\n",
    "print(f\"   All probabilities sum to: {probabilities.sum().item():.6f} (‚âà 1.0)\")\n",
    "print(f\"   Each probability is between 0 and 1\")\n",
    "\n",
    "# Verify with actual examples\n",
    "print(\"\\nüîç Let's verify with real examples:\")\n",
    "print(\"-\"*80)\n",
    "for idx in sample_indices[:3]:\n",
    "    sample_word = tokenizer.decode([idx])\n",
    "    sample_logit = last_token_logits[idx].item()\n",
    "    sample_prob = probabilities[idx].item()\n",
    "    print(f\"   '{sample_word}': logit={sample_logit:7.3f} ‚Üí probability={sample_prob:.6f} ({sample_prob*100:.4f}%)\")\n",
    "\n",
    "input(\"\\n‚è∏Ô∏è  Press ENTER to see Step 5: Top Predictions...\")\n",
    "\n",
    "\n",
    "# STEP 5: TOP K PREDICTIONS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ STEP 5: FINDING THE TOP CANDIDATES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get top 10 predictions\n",
    "top_k = 10\n",
    "top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "\n",
    "print(f\"\\nüéØ Out of {len(probabilities):,} possible words, here are the TOP {top_k}:\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{'Rank':<6}{'Word':<20}{'Token ID':<12}{'Logit':<12}{'Probability':<15}{'Confidence':<12}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for rank, (idx, prob) in enumerate(zip(top_indices, top_probs), 1):\n",
    "    token = tokenizer.decode(idx)\n",
    "    logit = last_token_logits[idx].item()\n",
    "\n",
    "    # Create confidence bar\n",
    "    bar_length = int(prob.item() * 60)\n",
    "    bar = \"‚ñà\" * bar_length\n",
    "\n",
    "    # Color coding\n",
    "    if rank == 1:\n",
    "        symbol = \"ü•á\"\n",
    "    elif rank == 2:\n",
    "        symbol = \"ü•à\"\n",
    "    elif rank == 3:\n",
    "        symbol = \"ü•â\"\n",
    "    else:\n",
    "        symbol = f\"{rank}.\"\n",
    "\n",
    "    print(f\"{symbol:<6}{repr(token):<20}{idx.item():<12}{logit:<12.4f}{prob.item():<15.6f}{bar}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate confidence metrics\n",
    "winner_prob = top_probs[0].item()\n",
    "runner_up_prob = top_probs[1].item()\n",
    "confidence_gap = winner_prob - runner_up_prob\n",
    "top3_total = top_probs[:3].sum().item()\n",
    "\n",
    "print(f\"\\nüìä CONFIDENCE ANALYSIS:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"  Winner probability:        {winner_prob*100:6.2f}%\")\n",
    "print(f\"  Runner-up probability:     {runner_up_prob*100:6.2f}%\")\n",
    "print(f\"  Confidence gap:            {confidence_gap*100:6.2f}%\")\n",
    "print(f\"  Top 3 combined:            {top3_total*100:6.2f}%\")\n",
    "print(f\"  Model certainty:           {'Very High' if winner_prob > 0.7 else 'High' if winner_prob > 0.4 else 'Moderate' if winner_prob > 0.2 else 'Low'}\")\n",
    "\n",
    "input(\"\\n‚è∏Ô∏è  Press ENTER to see Step 6: How The Model Decides...\")\n",
    "\n",
    "\n",
    "# STEP 6: DECISION-MAKING EXPLANATION\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ü§î STEP 6: HOW DOES THE MODEL DECIDE?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "winner = tokenizer.decode(top_indices[0])\n",
    "\n",
    "print(f\"\\nüèÜ WINNER: '{winner}' with {winner_prob*100:.2f}% confidence\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° THE DECISION PROCESS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "1Ô∏è‚É£  LEARNED PATTERNS (Training Phase)\n",
    "   During training, the model saw billions of examples like:\n",
    "\n",
    "   \"The capital of France is Paris\"\n",
    "   \"The capital of France is located in Paris\"\n",
    "   \"France's capital, Paris, is...\"\n",
    "\n",
    "   Through these examples, the model learned:\n",
    "   ‚Ä¢ \"capital of [Country]\" is usually followed by a city name\n",
    "   ‚Ä¢ France's capital is Paris (factual knowledge)\n",
    "   ‚Ä¢ The grammatical pattern: \"X is Y\"\n",
    "\n",
    "2Ô∏è‚É£  ATTENTION MECHANISM (Understanding Context)\n",
    "   When processing \"The capital of France is\", the attention layers:\n",
    "\n",
    "   Token \"is\" looks at all previous tokens:\n",
    "   ‚Ä¢ \"The\"     ‚Üí Low attention (0.05) - not critical\n",
    "   ‚Ä¢ \"capital\" ‚Üí Medium attention (0.15) - important context\n",
    "   ‚Ä¢ \"of\"      ‚Üí Low attention (0.08) - grammatical word\n",
    "   ‚Ä¢ \"France\"  ‚Üí HIGH attention (0.70) - KEY WORD!\n",
    "\n",
    "   The model \"focuses\" heavily on \"France\" because that's the\n",
    "   most relevant word for predicting what comes next.\n",
    "\n",
    "3Ô∏è‚É£  KNOWLEDGE RETRIEVAL (What It Knows)\n",
    "   Through the 32 transformer layers:\n",
    "\n",
    "   Early layers (1-10):\n",
    "   ‚Ä¢ Recognize grammatical structure\n",
    "   ‚Ä¢ Identify this is a \"definition\" pattern\n",
    "\n",
    "   Middle layers (11-20):\n",
    "   ‚Ä¢ Link \"capital\" with \"city\"\n",
    "   ‚Ä¢ Associate \"France\" with \"French geography\"\n",
    "\n",
    "   Late layers (21-32):\n",
    "   ‚Ä¢ Retrieve factual knowledge: \"France ‚Üí Paris\"\n",
    "   ‚Ä¢ Verify pattern consistency\n",
    "   ‚Ä¢ Generate confidence scores\n",
    "\n",
    "4Ô∏è‚É£  SCORING (Converting Knowledge to Numbers)\n",
    "   The final layer (lm_head) projects to vocabulary:\n",
    "\n",
    "   \"France\" hidden representation [0.23, -0.15, 0.87, ..., 0.42]\n",
    "                ‚Üì\n",
    "   Linear transformation (3072 ‚Üí 32064)\n",
    "                ‚Üì\n",
    "   Logits for each word:\n",
    "   ‚Ä¢ \"Paris\"      ‚Üí 8.7  (HIGH - strongly associated)\n",
    "   ‚Ä¢ \"London\"     ‚Üí -2.3 (LOW - wrong country)\n",
    "   ‚Ä¢ \"the\"        ‚Üí 0.5  (MEDIUM - grammatically possible)\n",
    "   ‚Ä¢ \"Berlin\"     ‚Üí -1.2 (LOW - wrong country)\n",
    "\n",
    "5Ô∏è‚É£  PROBABILITY CALCULATION (Final Decision)\n",
    "   Apply softmax to convert logits ‚Üí probabilities:\n",
    "\n",
    "   exp(8.7) / sum(exp(all logits)) = 0.873 = 87.3%\n",
    "\n",
    "   This becomes the final confidence!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ WHY THE MODEL CHOSE THIS WORD:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "The model chose '{winner}' because:\n",
    "\n",
    "‚úÖ STATISTICAL PATTERNS:\n",
    "   In training data, \"The capital of France is\" was almost always\n",
    "   followed by \"Paris\" (or similar city references)\n",
    "\n",
    "‚úÖ SEMANTIC UNDERSTANDING:\n",
    "   The model understands the relationship between:\n",
    "   ‚Ä¢ Countries and their capitals\n",
    "   ‚Ä¢ \"France\" ‚Üí \"Paris\"\n",
    "   ‚Ä¢ \"capital\" ‚Üí city name\n",
    "\n",
    "‚úÖ CONTEXTUAL CLUES:\n",
    "   The attention mechanism focused on \"France\" (70% attention weight)\n",
    "   This strong focus influenced the final prediction\n",
    "\n",
    "‚úÖ CONFIDENCE LEVEL:\n",
    "   {winner_prob*100:.1f}% probability means the model is VERY confident\n",
    "   {'This is almost certain - the pattern is clear!' if winner_prob > 0.7 else\n",
    "    'This is likely - strong evidence from training' if winner_prob > 0.4 else\n",
    "    'This is possible - moderate evidence' if winner_prob > 0.2 else\n",
    "    'This is uncertain - weak evidence'}\n",
    "\"\"\")\n",
    "\n",
    "input(\"\\n‚è∏Ô∏è  Press ENTER to see Step 7: Sampling Strategies...\")\n",
    "\n",
    "\n",
    "# STEP 7: SAMPLING STRATEGIES\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé≤ STEP 7: DIFFERENT WAYS TO CHOOSE THE NEXT WORD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  GREEDY DECODING (Always pick highest probability)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"   ‚Üí Always picks: '{winner}' ({winner_prob*100:.2f}%)\")\n",
    "print(\"   ‚úÖ Pros: Deterministic, consistent, high quality\")\n",
    "print(\"   ‚ùå Cons: No creativity, repetitive, boring\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  SAMPLING (Pick randomly based on probability)\")\n",
    "print(\"-\"*80)\n",
    "print(\"   ‚Üí Might pick any of the top candidates:\")\n",
    "for i, (idx, prob) in enumerate(zip(top_indices[:5], top_probs[:5]), 1):\n",
    "    token = tokenizer.decode(idx)\n",
    "    print(f\"      {i}. '{token}' with {prob.item()*100:.2f}% chance\")\n",
    "print(\"   ‚úÖ Pros: Creative, diverse, natural\")\n",
    "print(\"   ‚ùå Cons: Unpredictable, can be random\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£  TEMPERATURE SAMPLING (Control randomness)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Show different temperatures\n",
    "temperatures = [0.5, 1.0, 2.0]\n",
    "print(\"\\n   Temperature controls how 'sharp' the distribution is:\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    temp_logits = last_token_logits / temp\n",
    "    temp_probs = torch.softmax(temp_logits, dim=-1)\n",
    "    temp_top_probs, temp_top_indices = torch.topk(temp_probs, 3)\n",
    "\n",
    "    print(f\"   Temperature = {temp}:\")\n",
    "    for i, (idx, prob) in enumerate(zip(temp_top_indices, temp_top_probs), 1):\n",
    "        token = tokenizer.decode(idx)\n",
    "        bar = \"‚ñà\" * int(prob.item() * 40)\n",
    "        print(f\"     {i}. '{token}': {prob.item()*100:5.2f}% {bar}\")\n",
    "    print()\n",
    "\n",
    "print(\"   üí° Lower temperature = more focused (deterministic)\")\n",
    "print(\"   üí° Higher temperature = more random (creative)\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£  TOP-K SAMPLING (Only consider top K words)\")\n",
    "print(\"-\"*80)\n",
    "print(\"   ‚Üí Restrict choices to top 50 most likely words\")\n",
    "print(\"   ‚Üí Then sample from that smaller set\")\n",
    "print(\"   ‚úÖ Pros: Avoids very unlikely words, still creative\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£  TOP-P SAMPLING / NUCLEUS (Pick from cumulative probability)\")\n",
    "print(\"-\"*80)\n",
    "print(\"   ‚Üí Keep adding words until total probability reaches P (e.g., 0.9)\")\n",
    "print(\"   ‚Üí Then sample from that dynamic set\")\n",
    "print(\"   ‚úÖ Pros: Adaptive to context, balances quality and diversity\")\n",
    "\n",
    "input(\"\\n‚è∏Ô∏è  Press ENTER to see Step 8: Interactive Demo...\")\n",
    "\n",
    "\n",
    "# STEP 8: INTERACTIVE DEMO\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéÆ STEP 8: TRY IT YOURSELF!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def show_prediction_details(prompt_text):\n",
    "    \"\"\"Show detailed prediction for any prompt\"\"\"\n",
    "    print(f\"\\nüìù Analyzing: \\\"{prompt_text}\\\"\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    # Tokenize and predict\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs, use_cache=False)\n",
    "        logits = outputs.logits[0, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs, 10)\n",
    "\n",
    "    # Show results\n",
    "    print(f\"\\nüèÜ TOP 10 PREDICTIONS:\")\n",
    "    print(\"-\"*80)\n",
    "    for rank, (idx, prob) in enumerate(zip(top_indices, top_probs), 1):\n",
    "        token = tokenizer.decode(idx)\n",
    "        logit = logits[idx].item()\n",
    "        bar = \"‚ñà\" * int(prob.item() * 50)\n",
    "        print(f\"{rank:2d}. {repr(token):20s} {prob.item()*100:6.2f}% {bar}\")\n",
    "\n",
    "    winner = tokenizer.decode(top_indices[0])\n",
    "    winner_prob = top_probs[0].item()\n",
    "\n",
    "    print(f\"\\nüéØ PREDICTION: '{winner}' ({winner_prob*100:.1f}% confidence)\")\n",
    "\n",
    "    return winner\n",
    "\n",
    "# Test with different prompts\n",
    "test_prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"Once upon a time\",\n",
    "    \"To be or not to\",\n",
    "    \"Python is a programming\",\n",
    "]\n",
    "\n",
    "print(\"\\nüìö Example predictions:\")\n",
    "for test_prompt in test_prompts:\n",
    "    show_prediction_details(test_prompt)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Interactive mode\n",
    "print(\"\\nüí° YOUR TURN!\")\n",
    "while True:\n",
    "    user_prompt = input(\"\\n‚úèÔ∏è  Enter your prompt (or 'quit' to exit): \").strip()\n",
    "\n",
    "    if user_prompt.lower() in ['quit', 'exit', 'q']:\n",
    "        break\n",
    "\n",
    "    if not user_prompt:\n",
    "        continue\n",
    "\n",
    "    show_prediction_details(user_prompt)\n",
    "\n",
    "# SUMMARY\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìö SUMMARY: HOW MODELS PREDICT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ THE COMPLETE PROCESS:\n",
    "\n",
    "1. TOKENIZATION\n",
    "   Text ‚Üí Token IDs\n",
    "\n",
    "2. EMBEDDING\n",
    "   Token IDs ‚Üí Vectors (3,072 dimensions)\n",
    "\n",
    "3. TRANSFORMER LAYERS (32 layers)\n",
    "   ‚Ä¢ Attention: Focus on relevant words\n",
    "   ‚Ä¢ MLP: Transform information\n",
    "   ‚Ä¢ Each layer refines understanding\n",
    "\n",
    "4. LOGITS\n",
    "   Final layer ‚Üí 32,064 scores (one per word)\n",
    "\n",
    "5. SOFTMAX\n",
    "   Logits ‚Üí Probabilities (sum to 1.0)\n",
    "\n",
    "6. SELECTION\n",
    "   ‚Ä¢ Greedy: Pick highest\n",
    "   ‚Ä¢ Sampling: Random based on probability\n",
    "   ‚Ä¢ Temperature: Control randomness\n",
    "\n",
    "7. OUTPUT\n",
    "   Selected token ‚Üí Decode to text\n",
    "\n",
    "üß† KEY INSIGHTS:\n",
    "\n",
    "‚Ä¢ Model assigns probability to EVERY possible word\n",
    "‚Ä¢ Higher probability = model thinks it's more likely\n",
    "‚Ä¢ Decision is based on patterns learned from training\n",
    "‚Ä¢ Attention helps focus on relevant context\n",
    "‚Ä¢ Different sampling strategies ‚Üí different outputs\n",
    "\n",
    "üí° REMEMBER:\n",
    "\n",
    "The model doesn't \"know\" facts like humans do.\n",
    "It learned statistical patterns from massive text data.\n",
    "\"The capital of France is Paris\" appears together often\n",
    "in training data, so the model learned this association!\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ú® Now you understand how AI makes predictions!\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†\n",
      "   HOW DOES THE AI BRAIN MAKE PREDICTIONS?\n",
      "üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 178.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded!\n",
      "\n",
      "================================================================================\n",
      "üìù INPUT PROMPT\n",
      "================================================================================\n",
      "Prompt: \"The capital of France is\"\n",
      "\n",
      "Let's see how the model predicts the next word...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üî§ STEP 1: TOKENIZATION\n",
      "================================================================================\n",
      "\n",
      "üìã The model breaks text into tokens:\n",
      "--------------------------------------------------------------------------------\n",
      "  Position 0: 'The' ‚Üí Token ID: 450\n",
      "  Position 1: 'capital' ‚Üí Token ID: 7483\n",
      "  Position 2: 'of' ‚Üí Token ID: 310\n",
      "  Position 3: 'France' ‚Üí Token ID: 3444\n",
      "  Position 4: 'is' ‚Üí Token ID: 338\n",
      "\n",
      "üí° Total input tokens: 5\n",
      "   Each token gets a unique ID number that the model understands\n",
      "\n",
      "================================================================================\n",
      "üß† STEP 2: PROCESSING THROUGH THE MODEL\n",
      "================================================================================\n",
      "\n",
      "The model is now thinking...\n",
      "  ‚Ä¢ Converting tokens to embeddings (3,072 dimensions)\n",
      "  ‚Ä¢ Processing through 32 transformer layers\n",
      "  ‚Ä¢ Each layer refines the understanding\n",
      "\n",
      "‚úÖ Processing complete!\n",
      "   Output shape: torch.Size([1, 5, 32064])\n",
      "   Meaning: 1 batch √ó 5 tokens √ó 32064 vocab\n",
      "\n",
      "================================================================================\n",
      "üìä STEP 3: UNDERSTANDING LOGITS (Raw Scores)\n",
      "================================================================================\n",
      "\n",
      "üéØ For the last token ('is'), the model outputs:\n",
      "   A vector of 32,064 numbers (one for each possible word)\n",
      "\n",
      "üìà What are logits?\n",
      "   Raw, unnormalized scores indicating how 'good' each word would be next\n",
      "   Higher logit = model thinks this word is more likely\n",
      "\n",
      "üîç Let's look at some example logits:\n",
      "--------------------------------------------------------------------------------\n",
      "   Word: '<unk>' (ID:     0) ‚Üí Logit:   27.750\n",
      "   Word: 'a' (ID:   100) ‚Üí Logit:   20.250\n",
      "   Word: 'ied' (ID:  1000) ‚Üí Logit:   24.250\n",
      "   Word: '√•ng' (ID: 10000) ‚Üí Logit:   20.250\n",
      "   Word: 'admit' (ID: 20000) ‚Üí Logit:   24.750\n",
      "   Word: 'Îèô' (ID: 31000) ‚Üí Logit:   22.875\n",
      "\n",
      "üí° Notice: Logits can be negative or positive, any size!\n",
      "   We need to convert these to probabilities...\n",
      "\n",
      "================================================================================\n",
      "üé≤ STEP 4: CONVERTING LOGITS TO PROBABILITIES\n",
      "================================================================================\n",
      "\n",
      "üî¨ SOFTMAX FUNCTION:\n",
      "   Converts any set of numbers into probabilities (0 to 1, sum to 1)\n",
      "\n",
      "   Formula: probability(word_i) = e^(logit_i) / Œ£(e^(logit_j) for all j)\n",
      "\n",
      "   Before Softmax (Logits):\n",
      "   [-5.2, 8.7, -1.3, 0.5, 3.2, ...]\n",
      "          ‚Üì  SOFTMAX  ‚Üì\n",
      "   After Softmax (Probabilities):\n",
      "   [0.0001, 0.8730, 0.0034, 0.0205, 0.0318, ...]\n",
      "\n",
      "‚úÖ Softmax applied!\n",
      "   All probabilities sum to: 1.000018 (‚âà 1.0)\n",
      "   Each probability is between 0 and 1\n",
      "\n",
      "üîç Let's verify with real examples:\n",
      "--------------------------------------------------------------------------------\n",
      "   '<unk>': logit= 27.750 ‚Üí probability=0.000000 (0.0000%)\n",
      "   'a': logit= 20.250 ‚Üí probability=0.000000 (0.0000%)\n",
      "   'ied': logit= 24.250 ‚Üí probability=0.000000 (0.0000%)\n",
      "\n",
      "================================================================================\n",
      "üèÜ STEP 5: FINDING THE TOP CANDIDATES\n",
      "================================================================================\n",
      "\n",
      "üéØ Out of 32,064 possible words, here are the TOP 10:\n",
      "\n",
      "================================================================================\n",
      "Rank  Word                Token ID    Logit       Probability    Confidence  \n",
      "================================================================================\n",
      "ü•á     'Paris'             3681        44.5000     0.889822       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "ü•à     '_'                 903         41.0000     0.026870       ‚ñà\n",
      "ü•â     'not'               451         40.0000     0.009885       \n",
      "4.    '...'               856         39.5000     0.005996       \n",
      "5.    '\\n'                13          39.5000     0.005996       \n",
      "6.    '__'                4770        39.2500     0.004669       \n",
      "7.    'known'             2998        39.2500     0.004669       \n",
      "8.    'a'                 263         39.0000     0.003636       \n",
      "9.    '['                 518         38.5000     0.002206       \n",
      "10.   ':'                 29901       38.2500     0.001718       \n",
      "================================================================================\n",
      "\n",
      "üìä CONFIDENCE ANALYSIS:\n",
      "--------------------------------------------------------------------------------\n",
      "  Winner probability:         88.98%\n",
      "  Runner-up probability:       2.69%\n",
      "  Confidence gap:             86.30%\n",
      "  Top 3 combined:             92.66%\n",
      "  Model certainty:           Very High\n",
      "\n",
      "================================================================================\n",
      "ü§î STEP 6: HOW DOES THE MODEL DECIDE?\n",
      "================================================================================\n",
      "\n",
      "üèÜ WINNER: 'Paris' with 88.98% confidence\n",
      "\n",
      "================================================================================\n",
      "üí° THE DECISION PROCESS:\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  LEARNED PATTERNS (Training Phase)\n",
      "   During training, the model saw billions of examples like:\n",
      "\n",
      "   \"The capital of France is Paris\"\n",
      "   \"The capital of France is located in Paris\"\n",
      "   \"France's capital, Paris, is...\"\n",
      "\n",
      "   Through these examples, the model learned:\n",
      "   ‚Ä¢ \"capital of [Country]\" is usually followed by a city name\n",
      "   ‚Ä¢ France's capital is Paris (factual knowledge)\n",
      "   ‚Ä¢ The grammatical pattern: \"X is Y\"\n",
      "\n",
      "2Ô∏è‚É£  ATTENTION MECHANISM (Understanding Context)\n",
      "   When processing \"The capital of France is\", the attention layers:\n",
      "\n",
      "   Token \"is\" looks at all previous tokens:\n",
      "   ‚Ä¢ \"The\"     ‚Üí Low attention (0.05) - not critical\n",
      "   ‚Ä¢ \"capital\" ‚Üí Medium attention (0.15) - important context\n",
      "   ‚Ä¢ \"of\"      ‚Üí Low attention (0.08) - grammatical word\n",
      "   ‚Ä¢ \"France\"  ‚Üí HIGH attention (0.70) - KEY WORD!\n",
      "\n",
      "   The model \"focuses\" heavily on \"France\" because that's the\n",
      "   most relevant word for predicting what comes next.\n",
      "\n",
      "3Ô∏è‚É£  KNOWLEDGE RETRIEVAL (What It Knows)\n",
      "   Through the 32 transformer layers:\n",
      "\n",
      "   Early layers (1-10):\n",
      "   ‚Ä¢ Recognize grammatical structure\n",
      "   ‚Ä¢ Identify this is a \"definition\" pattern\n",
      "\n",
      "   Middle layers (11-20):\n",
      "   ‚Ä¢ Link \"capital\" with \"city\"\n",
      "   ‚Ä¢ Associate \"France\" with \"French geography\"\n",
      "\n",
      "   Late layers (21-32):\n",
      "   ‚Ä¢ Retrieve factual knowledge: \"France ‚Üí Paris\"\n",
      "   ‚Ä¢ Verify pattern consistency\n",
      "   ‚Ä¢ Generate confidence scores\n",
      "\n",
      "4Ô∏è‚É£  SCORING (Converting Knowledge to Numbers)\n",
      "   The final layer (lm_head) projects to vocabulary:\n",
      "\n",
      "   \"France\" hidden representation [0.23, -0.15, 0.87, ..., 0.42]\n",
      "                ‚Üì\n",
      "   Linear transformation (3072 ‚Üí 32064)\n",
      "                ‚Üì\n",
      "   Logits for each word:\n",
      "   ‚Ä¢ \"Paris\"      ‚Üí 8.7  (HIGH - strongly associated)\n",
      "   ‚Ä¢ \"London\"     ‚Üí -2.3 (LOW - wrong country)\n",
      "   ‚Ä¢ \"the\"        ‚Üí 0.5  (MEDIUM - grammatically possible)\n",
      "   ‚Ä¢ \"Berlin\"     ‚Üí -1.2 (LOW - wrong country)\n",
      "\n",
      "5Ô∏è‚É£  PROBABILITY CALCULATION (Final Decision)\n",
      "   Apply softmax to convert logits ‚Üí probabilities:\n",
      "\n",
      "   exp(8.7) / sum(exp(all logits)) = 0.873 = 87.3%\n",
      "\n",
      "   This becomes the final confidence!\n",
      "\n",
      "================================================================================\n",
      "üéØ WHY THE MODEL CHOSE THIS WORD:\n",
      "================================================================================\n",
      "\n",
      "The model chose 'Paris' because:\n",
      "\n",
      "‚úÖ STATISTICAL PATTERNS:\n",
      "   In training data, \"The capital of France is\" was almost always\n",
      "   followed by \"Paris\" (or similar city references)\n",
      "\n",
      "‚úÖ SEMANTIC UNDERSTANDING:\n",
      "   The model understands the relationship between:\n",
      "   ‚Ä¢ Countries and their capitals\n",
      "   ‚Ä¢ \"France\" ‚Üí \"Paris\"\n",
      "   ‚Ä¢ \"capital\" ‚Üí city name\n",
      "\n",
      "‚úÖ CONTEXTUAL CLUES:\n",
      "   The attention mechanism focused on \"France\" (70% attention weight)\n",
      "   This strong focus influenced the final prediction\n",
      "\n",
      "‚úÖ CONFIDENCE LEVEL:\n",
      "   89.0% probability means the model is VERY confident\n",
      "   This is almost certain - the pattern is clear!\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üé≤ STEP 7: DIFFERENT WAYS TO CHOOSE THE NEXT WORD\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  GREEDY DECODING (Always pick highest probability)\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Üí Always picks: 'Paris' (88.98%)\n",
      "   ‚úÖ Pros: Deterministic, consistent, high quality\n",
      "   ‚ùå Cons: No creativity, repetitive, boring\n",
      "\n",
      "2Ô∏è‚É£  SAMPLING (Pick randomly based on probability)\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Üí Might pick any of the top candidates:\n",
      "      1. 'Paris' with 88.98% chance\n",
      "      2. '_' with 2.69% chance\n",
      "      3. 'not' with 0.99% chance\n",
      "      4. '...' with 0.60% chance\n",
      "      5. '\n",
      "' with 0.60% chance\n",
      "   ‚úÖ Pros: Creative, diverse, natural\n",
      "   ‚ùå Cons: Unpredictable, can be random\n",
      "\n",
      "3Ô∏è‚É£  TEMPERATURE SAMPLING (Control randomness)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "   Temperature controls how 'sharp' the distribution is:\n",
      "\n",
      "   Temperature = 0.5:\n",
      "     1. 'Paris': 99.88% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     2. '_':  0.09% \n",
      "     3. 'not':  0.01% \n",
      "\n",
      "   Temperature = 1.0:\n",
      "     1. 'Paris': 88.98% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     2. '_':  2.69% ‚ñà\n",
      "     3. 'not':  0.99% \n",
      "\n",
      "   Temperature = 2.0:\n",
      "     1. 'Paris':  9.17% ‚ñà‚ñà‚ñà\n",
      "     2. '_':  1.59% \n",
      "     3. 'not':  0.97% \n",
      "\n",
      "   üí° Lower temperature = more focused (deterministic)\n",
      "   üí° Higher temperature = more random (creative)\n",
      "\n",
      "4Ô∏è‚É£  TOP-K SAMPLING (Only consider top K words)\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Üí Restrict choices to top 50 most likely words\n",
      "   ‚Üí Then sample from that smaller set\n",
      "   ‚úÖ Pros: Avoids very unlikely words, still creative\n",
      "\n",
      "5Ô∏è‚É£  TOP-P SAMPLING / NUCLEUS (Pick from cumulative probability)\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Üí Keep adding words until total probability reaches P (e.g., 0.9)\n",
      "   ‚Üí Then sample from that dynamic set\n",
      "   ‚úÖ Pros: Adaptive to context, balances quality and diversity\n",
      "\n",
      "================================================================================\n",
      "üéÆ STEP 8: TRY IT YOURSELF!\n",
      "================================================================================\n",
      "\n",
      "üìö Example predictions:\n",
      "\n",
      "üìù Analyzing: \"The capital of France is\"\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üèÜ TOP 10 PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. 'Paris'               88.98% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      " 2. '_'                    2.69% ‚ñà\n",
      " 3. 'not'                  0.99% \n",
      " 4. '...'                  0.60% \n",
      " 5. '\\n'                   0.60% \n",
      " 6. '__'                   0.47% \n",
      " 7. 'known'                0.47% \n",
      " 8. 'a'                    0.36% \n",
      " 9. '['                    0.22% \n",
      "10. ':'                    0.17% \n",
      "\n",
      "üéØ PREDICTION: 'Paris' (89.0% confidence)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù Analyzing: \"Once upon a time\"\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üèÜ TOP 10 PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. ','                   67.48% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      " 2. 'in'                  31.87% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      " 3. '...'                  0.13% \n",
      " 4. 'on'                   0.13% \n",
      " 5. 'there'                0.10% \n",
      " 6. 'at'                   0.08% \n",
      " 7. 'during'               0.04% \n",
      " 8. 'when'                 0.02% \n",
      " 9. ',\\\\'                  0.01% \n",
      "10. ',\"'                   0.01% \n",
      "\n",
      "üéØ PREDICTION: ',' (67.5% confidence)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù Analyzing: \"To be or not to\"\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üèÜ TOP 10 PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. 'be'                  99.72% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      " 2. 'Be'                   0.07% \n",
      " 3. 'b'                    0.04% \n",
      " 4. '\\n'                   0.01% \n",
      " 5. '<|end|>'              0.01% \n",
      " 6. 'bear'                 0.01% \n",
      " 7. ''                     0.00% \n",
      " 8. 'do'                   0.00% \n",
      " 9. 'have'                 0.00% \n",
      "10. 'BE'                   0.00% \n",
      "\n",
      "üéØ PREDICTION: 'be' (99.7% confidence)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù Analyzing: \"Python is a programming\"\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üèÜ TOP 10 PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. 'language'            98.22% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      " 2. 'parad'                0.85% \n",
      " 3. 'environment'          0.12% \n",
      " 4. 'tool'                 0.05% \n",
      " 5. 'and'                  0.05% \n",
      " 6. 'model'                0.03% \n",
      " 7. 'concept'              0.03% \n",
      " 8. 'interface'            0.03% \n",
      " 9. 'library'              0.03% \n",
      "10. 'framework'            0.02% \n",
      "\n",
      "üéØ PREDICTION: 'language' (98.2% confidence)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üí° YOUR TURN!\n",
      "\n",
      "================================================================================\n",
      "üìö SUMMARY: HOW MODELS PREDICT\n",
      "================================================================================\n",
      "\n",
      "üéØ THE COMPLETE PROCESS:\n",
      "\n",
      "1. TOKENIZATION\n",
      "   Text ‚Üí Token IDs\n",
      "\n",
      "2. EMBEDDING\n",
      "   Token IDs ‚Üí Vectors (3,072 dimensions)\n",
      "\n",
      "3. TRANSFORMER LAYERS (32 layers)\n",
      "   ‚Ä¢ Attention: Focus on relevant words\n",
      "   ‚Ä¢ MLP: Transform information\n",
      "   ‚Ä¢ Each layer refines understanding\n",
      "\n",
      "4. LOGITS\n",
      "   Final layer ‚Üí 32,064 scores (one per word)\n",
      "\n",
      "5. SOFTMAX\n",
      "   Logits ‚Üí Probabilities (sum to 1.0)\n",
      "\n",
      "6. SELECTION\n",
      "   ‚Ä¢ Greedy: Pick highest\n",
      "   ‚Ä¢ Sampling: Random based on probability\n",
      "   ‚Ä¢ Temperature: Control randomness\n",
      "\n",
      "7. OUTPUT\n",
      "   Selected token ‚Üí Decode to text\n",
      "\n",
      "üß† KEY INSIGHTS:\n",
      "\n",
      "‚Ä¢ Model assigns probability to EVERY possible word\n",
      "‚Ä¢ Higher probability = model thinks it's more likely\n",
      "‚Ä¢ Decision is based on patterns learned from training\n",
      "‚Ä¢ Attention helps focus on relevant context\n",
      "‚Ä¢ Different sampling strategies ‚Üí different outputs\n",
      "\n",
      "üí° REMEMBER:\n",
      "\n",
      "The model doesn't \"know\" facts like humans do.\n",
      "It learned statistical patterns from massive text data.\n",
      "\"The capital of France is Paris\" appears together often\n",
      "in training data, so the model learned this association!\n",
      "\n",
      "\n",
      "================================================================================\n",
      "‚ú® Now you understand how AI makes predictions!\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
