{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T15:28:42.166179Z",
     "start_time": "2025-12-26T15:25:15.883885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "ğŸ¬ BEHIND THE SCENES: How Language Models Generate Text\n",
    "A visual, step-by-step demonstration of what happens inside an LLM\n",
    "\n",
    "Perfect for non-technical audiences!\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "\n",
    "# SETUP: Load the Model\n",
    "print(\"\\n\" + \"ğŸ¬\" * 35)\n",
    "print(\"   WELCOME TO: INSIDE THE AI BRAIN\")\n",
    "print(\"ğŸ¬\" * 35 + \"\\n\")\n",
    "\n",
    "print(\"ğŸ“¦ Loading the AI model (Phi-3)...\")\n",
    "print(\"   Think of this as loading a very smart calculator that understands language\\n\")\n",
    "\n",
    "MODEL_PATH = \"/Users/somesh/How Transformer LLMs Work/Transformer Architecture/models/microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"Model loaded! Let's see what's inside...\\n\")\n",
    "\n",
    "# PART 1: MODEL ANATOMY\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ” PART 1: WHAT'S INSIDE THE MODEL?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ“Š MODEL STATISTICS:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  â€¢ Vocabulary Size:    {model.config.vocab_size:,} words/tokens\")\n",
    "print(f\"  â€¢ Embedding Dimension: {model.config.hidden_size:,} (how big each word's 'meaning vector' is)\")\n",
    "print(f\"  â€¢ Number of Layers:    {model.config.num_hidden_layers} (like floors in a building)\")\n",
    "print(f\"  â€¢ Attention Heads:     {model.config.num_attention_heads} (parallel thinking pathways)\")\n",
    "print(f\"  â€¢ Parameters:          ~3.8 billion tiny numbers that store knowledge\")\n",
    "print(\"\\nğŸ’¡ Analogy: Imagine a library with 32,064 books, stacked 32 floors high,\")\n",
    "print(\"     with 32 librarians on each floor helping you find information!\\n\")\n",
    "\n",
    "# Show the architecture\n",
    "print(\"\\nğŸ—ï¸  MODEL ARCHITECTURE (The Blueprint):\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  INPUT: \"The capital of France is\"                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 1: Token Embeddings                           â”‚\n",
    "â”‚  Converts words â†’ numbers â†’ meaning vectors         â”‚\n",
    "â”‚  Size: 32,064 vocabulary Ã— 3,072 dimensions         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 2: 32 Transformer Layers (The Brain)         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
    "â”‚  â”‚ Layer 1:  Self-Attention + Neural Network â”‚     â”‚\n",
    "â”‚  â”‚ Layer 2:  Self-Attention + Neural Network â”‚     â”‚\n",
    "â”‚  â”‚ Layer 3:  Self-Attention + Neural Network â”‚     â”‚\n",
    "â”‚  â”‚    ...    (each layer learns patterns)    â”‚     â”‚\n",
    "â”‚  â”‚ Layer 32: Self-Attention + Neural Network â”‚     â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 3: Language Model Head                        â”‚\n",
    "â”‚  Predicts next word from 32,064 possibilities       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  OUTPUT: \"Paris\" (with 87.3% confidence!)           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "input(\"\\nâ¸ï¸  Press ENTER to continue to the live demonstration...\")\n",
    "\n",
    "# PART 2: LIVE DEMONSTRATION\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ¥ PART 2: LIVE GENERATION - BEHIND THE SCENES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "print(f\"\\nğŸ“ INPUT PROMPT: \\\"{prompt}\\\"\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "\n",
    "# Step 1: Tokenization\n",
    "print(\"\\nğŸ”¤ STEP 1: TOKENIZATION (Breaking text into pieces)\")\n",
    "print(\"-\" * 80)\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "tokens = [tokenizer.decode([id]) for id in input_ids[0]]\n",
    "\n",
    "print(\"\\n  Original text: \\\"The capital of France is\\\"\")\n",
    "print(\"           â†“\")\n",
    "print(\"  Broken into tokens:\")\n",
    "for i, (token, token_id) in enumerate(zip(tokens, input_ids[0])):\n",
    "    print(f\"    Token {i+1}: '{token}' â†’ ID: {token_id.item()}\")\n",
    "\n",
    "print(f\"\\n    Total input tokens: {len(tokens)}\")\n",
    "print(\"\\n  ğŸ’¡ Why tokenization? Computers only understand numbers!\")\n",
    "print(\"       Each word/piece gets a unique ID from 0 to 32,063\")\n",
    "\n",
    "input(\"\\nâ¸ï¸  Press ENTER to see embeddings...\")\n",
    "\n",
    "# Step 2: Embeddings\n",
    "print(\"\\nğŸ“Š STEP 2: EMBEDDINGS (Converting IDs to meaning vectors)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model.model.embed_tokens(input_ids)\n",
    "\n",
    "print(f\"\\n  Shape: {embeddings.shape}\")\n",
    "print(f\"         â†“\")\n",
    "print(f\"  Batch size: {embeddings.shape[0]} (how many sentences)\")\n",
    "print(f\"  Tokens:     {embeddings.shape[1]} (words in sentence)\")\n",
    "print(f\"  Dimensions: {embeddings.shape[2]} (size of each meaning vector)\")\n",
    "\n",
    "print(f\"\\n  Example: First token '{tokens[0]}' becomes a vector of 3,072 numbers:\")\n",
    "print(f\"  [{embeddings[0, 0, 0]:.4f}, {embeddings[0, 0, 1]:.4f}, {embeddings[0, 0, 2]:.4f}, ... {embeddings[0, 0, -1]:.4f}]\")\n",
    "\n",
    "print(\"\\n ğŸ’¡Think of each vector as coordinates in a 3,072-dimensional space!\")\n",
    "print(\"     Similar words (like 'king' and 'queen') are close together\")\n",
    "\n",
    "input(\"\\nâ¸ï¸ Press ENTER to see the model thinking...\")\n",
    "\n",
    "# Step 3: Through the layers\n",
    "print(\"\\nğŸ§  STEP 3: PROCESSING THROUGH 32 LAYERS (The AI is thinking!)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n  Each layer does two things:\")\n",
    "print(\"    1ï¸âƒ£  ATTENTION: 'Which words should I focus on?'\")\n",
    "print(\"    2ï¸âƒ£  PROCESSING: 'What does this mean?'\")\n",
    "print(\"\\n  Let's watch as information flows through the layers...\\n\")\n",
    "\n",
    "layer_outputs = []\n",
    "hidden_states = embeddings.clone()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        seq_len = hidden_states.shape[1]\n",
    "        position_ids = torch.arange(\n",
    "            seq_len,\n",
    "            device=hidden_states.device\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        if i < 3 or i == 15 or i == 31:\n",
    "            print(f\"  ğŸ”„ Layer {i+1}/32 processing...\")\n",
    "        elif i == 3:\n",
    "            print(\"  â© Layers 4â€“31 processing...\")\n",
    "\n",
    "        layer_output = layer(\n",
    "            hidden_states,\n",
    "            attention_mask=None,\n",
    "            position_ids=position_ids,\n",
    "            use_cache=False,\n",
    "            output_attentions=False,\n",
    "        )\n",
    "\n",
    "        hidden_states = layer_output[0]\n",
    "\n",
    "        if i in [0, 15, 31]:\n",
    "            layer_outputs.append(hidden_states.clone())\n",
    "\n",
    "print(\"\\n âœ…All 32 layers complete!\")\n",
    "print(\"\\n ğŸ’¡Each layer refines the understanding:\")\n",
    "print(\"     Early layers (1-10):   Learn grammar and syntax\")\n",
    "print(\"     Middle layers (11-20): Learn meaning and relationships\")\n",
    "print(\"     Late layers (21-32):   Learn specific knowledge (like 'capital of France = Paris')\")\n",
    "\n",
    "input(\"\\nâ¸ï¸ Press ENTER to see the final prediction...\")\n",
    "\n",
    "# Step 4: Generate next token\n",
    "print(\"\\nğŸ¯ STEP 4: PREDICTING THE NEXT WORD\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get final output\n",
    "    outputs = model(input_ids, use_cache=False)\n",
    "    logits = outputs.logits[0, -1, :]  # Last token's predictions\n",
    "\n",
    "    # Get probabilities\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # Get top 10 predictions\n",
    "    top_k = 10\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "\n",
    "print(\"\\n  The model's brain has 32,064 neurons, each voting for a different word.\")\n",
    "print(\"  Let's see the TOP 10 candidates:\\n\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  {'Rank':<6}{'Word':<20}{'Confidence':<15}{'Probability':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for rank, (idx, prob) in enumerate(zip(top_indices, top_probs), 1):\n",
    "    token = tokenizer.decode(idx)\n",
    "    bar_length = int(prob.item() * 50)\n",
    "    bar = \"â–ˆ\" * bar_length\n",
    "    print(f\"  {rank:<6}{repr(token):<20}{prob.item()*100:>6.2f}%{'':>8}{bar}\")\n",
    "\n",
    "winner = tokenizer.decode(top_indices[0])\n",
    "winner_prob = top_probs[0].item() * 100\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"\\n  ğŸ† WINNER: '{winner}' with {winner_prob:.2f}% confidence!\")\n",
    "print(f\"\\n  Full answer: \\\"{prompt}{winner}\\\"\")\n",
    "\n",
    "input(\"\\nâ¸ï¸  Press ENTER to generate a full sentence...\")\n",
    "\n",
    "# PART 3: FULL GENERATION\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ¬ PART 3: GENERATING MULTIPLE WORDS (AUTOREGRESSIVE GENERATION)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "prompt2 = \"The capital of France is\"\n",
    "max_tokens = 20\n",
    "\n",
    "print(f\"\\nğŸ“ Starting prompt: \\\"{prompt2}\\\"\")\n",
    "print(\"\\n  The model will generate one word at a time, adding each new word\")\n",
    "print(\"  to the prompt and predicting the next word. Let's watch!\\n\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "input_ids = tokenizer(prompt2, return_tensors=\"pt\").input_ids\n",
    "generated_ids = input_ids.clone()\n",
    "generated_text = prompt2\n",
    "\n",
    "print(f\"  Starting: \\\"{generated_text}\\\"\")\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(max_tokens):\n",
    "        # Generate next token\n",
    "        outputs = model(generated_ids, use_cache=False)\n",
    "        next_token_id = outputs.logits[0, -1, :].argmax(-1)\n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "\n",
    "        # Get top 3 for this step\n",
    "        probs = torch.softmax(outputs.logits[0, -1, :], dim=-1)\n",
    "        top3_probs, top3_indices = torch.topk(probs, 3)\n",
    "\n",
    "        # Show the thinking\n",
    "        print(f\"  Step {step+1}:\")\n",
    "        print(f\"    Current text: \\\"{generated_text}\\\"\")\n",
    "        print(f\"    Top 3 next words:\")\n",
    "        for i, (idx, prob) in enumerate(zip(top3_indices, top3_probs), 1):\n",
    "            token = tokenizer.decode(idx)\n",
    "            marker = \"âœ“\" if i == 1 else \" \"\n",
    "            print(f\"      {marker} {i}. '{token}' ({prob.item()*100:.1f}%)\")\n",
    "        print(f\"    â†’ Chose: '{next_token}'\")\n",
    "\n",
    "        # Update\n",
    "        generated_text += next_token\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "\n",
    "        print(f\"    New text: \\\"{generated_text}\\\"\")\n",
    "        print()\n",
    "\n",
    "        # Stop at end of sentence or period\n",
    "        if next_token_id.item() == tokenizer.eos_token_id or \".\" in next_token:\n",
    "            print(f\"âœ… Stopped at step {step+1} (found ending)\")\n",
    "            break\n",
    "\n",
    "        time.sleep(0.5)  # Pause for effect\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"\\n  ğŸ‰ FINAL RESULT: \\\"{generated_text}\\\"\")\n",
    "\n",
    "# PART 4: KEY INSIGHTS\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ’¡ KEY INSIGHTS: WHAT DID WE LEARN?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. ğŸ”¤ TOKENIZATION\n",
    "   Text is broken into pieces (tokens) and converted to numbers.\n",
    "   The model has a vocabulary of 32,064 different tokens.\n",
    "\n",
    "2. ğŸ“Š EMBEDDINGS\n",
    "   Each token becomes a vector of 3,072 numbers that captures its meaning.\n",
    "   Similar words have similar vectors.\n",
    "\n",
    "3. ğŸ§  32 TRANSFORMER LAYERS\n",
    "   Each layer has:\n",
    "   â€¢ ATTENTION: Looks at relationships between words\n",
    "   â€¢ MLP: Processes and transforms information\n",
    "\n",
    "   Early layers learn grammar, late layers learn facts!\n",
    "\n",
    "4. ğŸ¯ PREDICTION\n",
    "   The final layer outputs scores for all 32,064 possible next words.\n",
    "   We pick the highest score (or sample based on probabilities).\n",
    "\n",
    "5. ğŸ” AUTOREGRESSIVE\n",
    "   To generate multiple words, we:\n",
    "   â†’ Predict one word\n",
    "   â†’ Add it to the input\n",
    "   â†’ Predict the next word\n",
    "   â†’ Repeat!\n",
    "\n",
    "6. ğŸ“ˆ CONFIDENCE\n",
    "   The model assigns probability to each possible next word.\n",
    "   Higher probability = more confident!\n",
    "\n",
    "7. ğŸ² RANDOMNESS (Optional)\n",
    "   â€¢ Greedy: Always pick highest probability (deterministic)\n",
    "   â€¢ Sampling: Pick randomly based on probabilities (creative)\n",
    "   â€¢ Temperature: Control randomness (higher = more random)\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nâœ¨ THE MAGIC: The model learned all this from reading massive amounts of text!\")\n",
    "print(\"   It discovered patterns about language, grammar, facts, and reasoning\")\n",
    "print(\"   just by trying to predict the next word billions of times!\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# INTERACTIVE DEMO\n",
    "print(\"\\n\" + \"ğŸ®\" * 35)\n",
    "print(\"YOUR TURN: TRY YOUR OWN PROMPT!\")\n",
    "print(\"ğŸ®\" * 35 + \"\\n\")\n",
    "\n",
    "while True:\n",
    "    user_prompt = input(\"\\nâœï¸ Enter your prompt (or 'quit' to exit): \").strip()\n",
    "\n",
    "    if user_prompt.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"\\nğŸ‘‹ Thanks for exploring! Now you understand how LLMs work!\\n\")\n",
    "        break\n",
    "\n",
    "    if not user_prompt:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸ¬ Generating response for: \\\"{user_prompt}\\\"\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    input_ids = tokenizer(user_prompt, return_tensors=\"pt\").input_ids\n",
    "    generated_ids = input_ids.clone()\n",
    "    generated_text = \"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(50):\n",
    "            outputs = model(generated_ids, use_cache=False)\n",
    "            next_token_id = outputs.logits[0, -1, :].argmax(-1)\n",
    "            token = tokenizer.decode(next_token_id)\n",
    "            generated_text += token\n",
    "\n",
    "            print(token, end='', flush=True)\n",
    "\n",
    "            generated_ids = torch.cat([\n",
    "                generated_ids,\n",
    "                next_token_id.unsqueeze(0).unsqueeze(0)\n",
    "            ], dim=1)\n",
    "\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            time.sleep(0.05)\n",
    "\n",
    "    print(f\"\\n\\nâœ… Complete response: {user_prompt}{generated_text}\")\n",
    "    print(\"-\" * 80)"
   ],
   "id": "716ba984048dba3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬\n",
      "   WELCOME TO: INSIDE THE AI BRAIN\n",
      "ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬ğŸ¬\n",
      "\n",
      "ğŸ“¦ Loading the AI model (Phi-3)...\n",
      "   Think of this as loading a very smart calculator that understands language\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 209.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded! Let's see what's inside...\n",
      "\n",
      "================================================================================\n",
      "ğŸ” PART 1: WHAT'S INSIDE THE MODEL?\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š MODEL STATISTICS:\n",
      "--------------------------------------------------------------------------------\n",
      "  â€¢ Vocabulary Size:    32,064 words/tokens\n",
      "  â€¢ Embedding Dimension: 3,072 (how big each word's 'meaning vector' is)\n",
      "  â€¢ Number of Layers:    32 (like floors in a building)\n",
      "  â€¢ Attention Heads:     32 (parallel thinking pathways)\n",
      "  â€¢ Parameters:          ~3.8 billion tiny numbers that store knowledge\n",
      "\n",
      "ğŸ’¡ Analogy: Imagine a library with 32,064 books, stacked 32 floors high,\n",
      "     with 32 librarians on each floor helping you find information!\n",
      "\n",
      "\n",
      "ğŸ—ï¸  MODEL ARCHITECTURE (The Blueprint):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  INPUT: \"The capital of France is\"                  â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "                        â†“\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  STEP 1: Token Embeddings                           â”‚\n",
      "â”‚  Converts words â†’ numbers â†’ meaning vectors         â”‚\n",
      "â”‚  Size: 32,064 vocabulary Ã— 3,072 dimensions         â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "                        â†“\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  STEP 2: 32 Transformer Layers (The Brain)         â”‚\n",
      "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
      "â”‚  â”‚ Layer 1:  Self-Attention + Neural Network â”‚     â”‚\n",
      "â”‚  â”‚ Layer 2:  Self-Attention + Neural Network â”‚     â”‚\n",
      "â”‚  â”‚ Layer 3:  Self-Attention + Neural Network â”‚     â”‚\n",
      "â”‚  â”‚    ...    (each layer learns patterns)    â”‚     â”‚\n",
      "â”‚  â”‚ Layer 32: Self-Attention + Neural Network â”‚     â”‚\n",
      "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "                        â†“\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  STEP 3: Language Model Head                        â”‚\n",
      "â”‚  Predicts next word from 32,064 possibilities       â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "                        â†“\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  OUTPUT: \"Paris\" (with 87.3% confidence!)           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ¥ PART 2: LIVE GENERATION - BEHIND THE SCENES\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ INPUT PROMPT: \"The capital of France is\"\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ”¤ STEP 1: TOKENIZATION (Breaking text into pieces)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Original text: \"The capital of France is\"\n",
      "           â†“\n",
      "  Broken into tokens:\n",
      "    Token 1: 'The' â†’ ID: 450\n",
      "    Token 2: 'capital' â†’ ID: 7483\n",
      "    Token 3: 'of' â†’ ID: 310\n",
      "    Token 4: 'France' â†’ ID: 3444\n",
      "    Token 5: 'is' â†’ ID: 338\n",
      "\n",
      "    Total input tokens: 5\n",
      "\n",
      "  ğŸ’¡ Why tokenization? Computers only understand numbers!\n",
      "       Each word/piece gets a unique ID from 0 to 32,063\n",
      "\n",
      "ğŸ“Š STEP 2: EMBEDDINGS (Converting IDs to meaning vectors)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Shape: torch.Size([1, 5, 3072])\n",
      "         â†“\n",
      "  Batch size: 1 (how many sentences)\n",
      "  Tokens:     5 (words in sentence)\n",
      "  Dimensions: 3072 (size of each meaning vector)\n",
      "\n",
      "  Example: First token 'The' becomes a vector of 3,072 numbers:\n",
      "  [0.0184, 0.0109, -0.0113, ... 0.0315]\n",
      "\n",
      " ğŸ’¡Think of each vector as coordinates in a 3,072-dimensional space!\n",
      "     Similar words (like 'king' and 'queen') are close together\n",
      "\n",
      "ğŸ§  STEP 3: PROCESSING THROUGH 32 LAYERS (The AI is thinking!)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Each layer does two things:\n",
      "    1ï¸âƒ£  ATTENTION: 'Which words should I focus on?'\n",
      "    2ï¸âƒ£  PROCESSING: 'What does this mean?'\n",
      "\n",
      "  Let's watch as information flows through the layers...\n",
      "\n",
      "  ğŸ”„ Layer 1/32 processing...\n",
      "  ğŸ”„ Layer 2/32 processing...\n",
      "  ğŸ”„ Layer 3/32 processing...\n",
      "  â© Layers 4â€“31 processing...\n",
      "  ğŸ”„ Layer 16/32 processing...\n",
      "  ğŸ”„ Layer 32/32 processing...\n",
      "\n",
      " âœ…All 32 layers complete!\n",
      "\n",
      " ğŸ’¡Each layer refines the understanding:\n",
      "     Early layers (1-10):   Learn grammar and syntax\n",
      "     Middle layers (11-20): Learn meaning and relationships\n",
      "     Late layers (21-32):   Learn specific knowledge (like 'capital of France = Paris')\n",
      "\n",
      "ğŸ¯ STEP 4: PREDICTING THE NEXT WORD\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  The model's brain has 32,064 neurons, each voting for a different word.\n",
      "  Let's see the TOP 10 candidates:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "  Rank  Word                Confidence     Probability    \n",
      "--------------------------------------------------------------------------------\n",
      "  1     'Paris'              88.98%        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  2     '_'                   2.69%        â–ˆ\n",
      "  3     'not'                 0.99%        \n",
      "  4     '...'                 0.60%        \n",
      "  5     '\\n'                  0.60%        \n",
      "  6     '__'                  0.47%        \n",
      "  7     'known'               0.47%        \n",
      "  8     'a'                   0.36%        \n",
      "  9     '['                   0.22%        \n",
      "  10    ':'                   0.17%        \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  ğŸ† WINNER: 'Paris' with 88.98% confidence!\n",
      "\n",
      "  Full answer: \"The capital of France isParis\"\n",
      "\n",
      "================================================================================\n",
      "ğŸ¬ PART 3: GENERATING MULTIPLE WORDS (AUTOREGRESSIVE GENERATION)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Starting prompt: \"The capital of France is\"\n",
      "\n",
      "  The model will generate one word at a time, adding each new word\n",
      "  to the prompt and predicting the next word. Let's watch!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "  Starting: \"The capital of France is\"\n",
      "\n",
      "  Step 1:\n",
      "    Current text: \"The capital of France is\"\n",
      "    Top 3 next words:\n",
      "      âœ“ 1. 'Paris' (89.0%)\n",
      "        2. '_' (2.7%)\n",
      "        3. 'not' (1.0%)\n",
      "    â†’ Chose: 'Paris'\n",
      "    New text: \"The capital of France isParis\"\n",
      "\n",
      "  Step 2:\n",
      "    Current text: \"The capital of France isParis\"\n",
      "    Top 3 next words:\n",
      "      âœ“ 1. '.' (86.4%)\n",
      "        2. ',' (9.1%)\n",
      "        3. '.\"' (2.0%)\n",
      "    â†’ Chose: '.'\n",
      "    New text: \"The capital of France isParis.\"\n",
      "\n",
      "âœ… Stopped at step 2 (found ending)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  ğŸ‰ FINAL RESULT: \"The capital of France isParis.\"\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¡ KEY INSIGHTS: WHAT DID WE LEARN?\n",
      "================================================================================\n",
      "\n",
      "1. ğŸ”¤ TOKENIZATION\n",
      "   Text is broken into pieces (tokens) and converted to numbers.\n",
      "   The model has a vocabulary of 32,064 different tokens.\n",
      "\n",
      "2. ğŸ“Š EMBEDDINGS\n",
      "   Each token becomes a vector of 3,072 numbers that captures its meaning.\n",
      "   Similar words have similar vectors.\n",
      "\n",
      "3. ğŸ§  32 TRANSFORMER LAYERS\n",
      "   Each layer has:\n",
      "   â€¢ ATTENTION: Looks at relationships between words\n",
      "   â€¢ MLP: Processes and transforms information\n",
      "\n",
      "   Early layers learn grammar, late layers learn facts!\n",
      "\n",
      "4. ğŸ¯ PREDICTION\n",
      "   The final layer outputs scores for all 32,064 possible next words.\n",
      "   We pick the highest score (or sample based on probabilities).\n",
      "\n",
      "5. ğŸ” AUTOREGRESSIVE\n",
      "   To generate multiple words, we:\n",
      "   â†’ Predict one word\n",
      "   â†’ Add it to the input\n",
      "   â†’ Predict the next word\n",
      "   â†’ Repeat!\n",
      "\n",
      "6. ğŸ“ˆ CONFIDENCE\n",
      "   The model assigns probability to each possible next word.\n",
      "   Higher probability = more confident!\n",
      "\n",
      "7. ğŸ² RANDOMNESS (Optional)\n",
      "   â€¢ Greedy: Always pick highest probability (deterministic)\n",
      "   â€¢ Sampling: Pick randomly based on probabilities (creative)\n",
      "   â€¢ Temperature: Control randomness (higher = more random)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ¨ THE MAGIC: The model learned all this from reading massive amounts of text!\n",
      "   It discovered patterns about language, grammar, facts, and reasoning\n",
      "   just by trying to predict the next word billions of times!\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®\n",
      "YOUR TURN: TRY YOUR OWN PROMPT!\n",
      "ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®\n",
      "\n",
      "\n",
      "ğŸ‘‹ Thanks for exploring! Now you understand how LLMs work!\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
